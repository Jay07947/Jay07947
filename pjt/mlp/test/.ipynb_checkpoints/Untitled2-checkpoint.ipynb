{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ed89f5ec9241>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'score_total'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'strong'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindChildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'em'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtotal_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# print(html) #해당 페이지에서 원하는 영역 구조 및 태그 확인\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests # 파이썬에서  http 요청을 보낼 때 쓰는 라이브러리\n",
    "from bs4 import BeautifulSoup #html에서 정보를 간단하게 빼오기 위해 사용할 라이브러리 \n",
    "from datetime import datetime\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "test_url = \"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=136990&type=after&page=1\"\n",
    "resp = requests.get(test_url)\n",
    "html = BeautifulSoup(resp.content, 'html.parser')\n",
    "result = html.find('div', {'class':'score_total'}).find('strong').findChildren('em')[1].getText()\n",
    "total_count = int(result.replace(',', ''))\n",
    "# print(html) #해당 페이지에서 원하는 영역 구조 및 태그 확인\n",
    "\n",
    "def get_data(url):\n",
    "    resp = requests.get(url)\n",
    "    html = BeautifulSoup(resp.content, 'html.parser')\n",
    "    score_result = html.find('div', {'class': 'score_result'})\n",
    "    lis = score_result.findAll('li')\n",
    "    for li in lis:\n",
    "        nickname = li.findAll('a')[0].find('span').getText()\n",
    "        created_at = datetime.strptime(li.find('dt').findAll('em')[-1].getText(), \"%Y.%m.%d %H:%M\")\n",
    "\n",
    "        review_text = li.find('p').getText()\n",
    "        score = li.find('em').getText()\n",
    "        btn_likes = li.find('div', {'class': 'btn_area'}).findAll('span')\n",
    "        like = btn_likes[1].getText()\n",
    "        dislike = btn_likes[3].getText()\n",
    "\n",
    "        watch_movie = li.find('span', {'class':'ico_viewer'})\n",
    "\n",
    "        # 간단하게 프린트만 했습니다.\n",
    "        print(nickname, review_text, score, like, dislike, created_at, watch_movie and True or False)\n",
    "        \n",
    "        \n",
    "\n",
    "for i in range(1, int(total_count / 10) + 1):\n",
    "    url = test_url + '&page=' + str(i)\n",
    "    print('url: \"' + url + '\" is parsing....')\n",
    "    get_data(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  8.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import pandas  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_review(pageNo):\n",
    "    result = []\n",
    "\n",
    "    url = 'https://movie.naver.com/movie/point/af/list.nhn?&page={}'.format(pageNo)\n",
    "    response = requests.get(url) \n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'lxml') \n",
    "    trs = soup.select(\"table.list_netizen > tbody > tr\")\n",
    "    #soup.select('상위태그명 > 하위태그명 > 하위태그명')\n",
    "    \n",
    "    \n",
    "    for tr in trs:\n",
    "        tds = tr.select('td')\n",
    "        number = tds[0].text \n",
    "        author = tds[2].select_one('a').text \n",
    "        day = tds[2].text[len(author):] \n",
    "        title = tds[1].select_one('a').text \n",
    "        star = tds[1].select_one('em').text \n",
    "        content = tds[1].text[len(title) +17 : -3].strip() \n",
    "\n",
    "        result.append([number, author, day, title, star, content]) \n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_reviews(start = 1, end = 10): \n",
    "    if start < 1:\n",
    "        start = 1\n",
    "    if end > 1000:\n",
    "        end = 1000\n",
    "\n",
    "    result = []\n",
    "    for i in tqdm(range(start, end+1)):\n",
    "        result.extend(get_review(i))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    reviewData = get_reviews(1, 20)\n",
    "\n",
    "    ##엑셀파일\n",
    "    col = ['리뷰번호', '작성자', '작성날짜', '제목', '별점', '내용']\n",
    "    dataFrame = pandas.DataFrame(reviewData, columns= col)\n",
    "    dataFrame.to_excel(\"Movie_data.xlsx\", \n",
    "                       sheet_name= '네이버 영화 네티즌 리뷰', \n",
    "                       header=True, \n",
    "                       startrow=0, encoding='UTF-8')\n",
    "    \n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# req = requests.get(\"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=190568&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page=1\")\n",
    "# html = req.text\n",
    "\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# for item, i in zip(soup.select(\".score_result .score_reple\"), range(1,6)):\n",
    "#     result = item.text\n",
    "#     result = result.replace('\\t','')\n",
    "#     result = result.replace('\\r','')\n",
    "#     result = result.replace('\\n','')\n",
    "#     print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
